# Reinforcement-Learning

<h1>Optimizing collective fieldtaxis of swarming agents through reinforcement learnng</h1>
<h3>Introduction</h3><br>
The swarming of animal groups has fascinated scientists in fields ranging from
biology to physics to engineering. The complex swarming patterns often arise from
simple interactions between individuals to the benefit of the collective whole. Here we
show that a machine-learning technique can be employed to tune these underlying
parameters and optimize the resulting performance.
In this project we try to simulate a group of fish which try to find preferred regions
in a sea or ocean where there might be less light so that the predator doesnâ€™t find them.
They do it collectively by interacting with each other on an individual level that benefits
the whole swarm.
<br>
<h3>Algorithm and Simulations</h3>
Part - I
<br><br>
Firstly I casted a static light field using the equation<br><br>
<h4>
  ğ¹(ğ‘‹) =ğ‘˜âˆ‘[ğ´<sub>ğ‘˜</sub>ğ‘ğ‘œğ‘ (ğ‘˜. ğ‘‹) + ğµğ‘˜ğ‘ ğ‘–ğ‘›(ğ‘˜. ğ‘‹)]
</h4>
<br>
We do the sum over wave vectors, k =(ğ‘¥,ğ‘˜ğ‘¦) ğ‘˜
ğ‘˜ which runs through
ğ‘¥
,ğ‘˜
ğ‘¦
2Ï€
ğ¿
We get ğ´ and using gaussian normal distribution with mean of 0.5 and standard ğ‘˜
ğµğ‘˜
deviation 0.05
Then I initialized positions of the agents uniformly within a circle of radius R = ,
ğ‘
Ï€
where N = number of agents , here N = 16
In fishes we can see a coherent flocking happens by interacting through repulsion at
short range ğ‘Ÿ , orientation at intermediate range , and attraction at long range .
ğ‘Ÿ
ğ‘Ÿ
ğ‘œ
ğ‘Ÿ
ğ‘
With each time step âˆ†ğ‘¡, we update the positions and velocities as
ğ‘¥ =
ğ‘–
(ğ‘¡ + âˆ†ğ‘¡) ğ‘¥
ğ‘–
(ğ‘¡) +
1
2
[ğ‘£
ğ‘–
(ğ‘¡) + ğ‘£
ğ‘–
(ğ‘¡ + âˆ†ğ‘¡)]
ğ‘£ = ]
ğ‘–
(ğ‘¡ + âˆ†ğ‘¡) [ğ‘£ğ‘šğ‘ğ‘¥
ğ¹(ğ‘¥
ğ‘–
(ğ‘¡))] ğ‘…
ğ‘›ğ‘œğ‘–ğ‘ ğ‘’
ğ‘‘
ğ‘–
(ğ‘¡)
The position of particles changes accordingly with the velocity vector
And the magnitude of velocity of particles changes with the amount light received
And the direction of the particles changes accordingly with the following conditions
i) If there are any neighbors within the zone of repulsion ğ‘Ÿ , they need to repel so
ğ‘Ÿ
ii) If there neighbors in the zone of repulsion but within zone of orientation ğ‘Ÿ ,then they
ğ‘œ
need orient in the same direction. If within zone of attraction ğ‘Ÿ , then they need to
ğ‘
attract so
iii) If there are no neighbors within zone of attraction ğ‘Ÿ , then there will be no change in
ğ‘
the direction so
In training we allow the agents to move up to time t = ğ‘¡ where = 100 in our case *
ğ‘¡
*
We can see the following path by the agents
Here we chose ğ‘Ÿ = 1.95 , = 2.05 & = 1.00
ğ‘œ
ğ‘Ÿ
ğ‘
ğ‘Ÿ
ğ‘Ÿ
We need to optimize ğ‘Ÿ using reinforcement learning so that the agents can find the
ğ‘œ
, ğ‘Ÿ
ğ‘
darkest spot easily.
For this we run the training algorithm for ğ‘ times indexed by .
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
Î± = 1, 2,.... ğ‘
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
We take the average field intensity perceived by the agents so that we can use this as
reward
Then we define the reward as
Q = max{ 0 ,ğ‘“(0) - ğ‘“(ğ‘¡ }
*
)
So, if f(t) decreases over time ğ‘¡ then the reward is going to increase. *
At Î±-th training session with initial positions and velocities of agents, we evaluate the
rewards at three nearby points: ğ‘„ at ( ), at and at
0
ğ‘Ÿ
ğ‘œ
(Î±)
, ğ‘Ÿ
ğ‘
(Î±) ğ‘„1
(ğ‘Ÿ
ğ‘œ
(Î±) âˆ’ Î´, ğ‘Ÿ
ğ‘
(Î±)
) ğ‘„2
(ğ‘Ÿ with deviation . And we update the parameters as
ğ‘œ
(Î±)
, ğ‘Ÿ
ğ‘
(Î±) + Î´) Î´ = (Î± + 1)
âˆ’1/4
After many iterations we can see how ğ‘Ÿ are changing and get the optimised .
ğ‘œ
, ğ‘Ÿ
ğ‘
ğ‘Ÿ
ğ‘œ
, ğ‘Ÿ
ğ‘
We can see that ğ‘Ÿ are approaching 1.75, 1.68.
ğ‘œ
, ğ‘Ÿ
ğ‘
Part - II
We need to train our agents with allowed continuum values of ro and ra i.e., [1,3] .
We need to train on each and every pair of ro and ra values and take the average
reward over different static fields.
After training over 1600 fields we got
In this we can see that ra greater than ro will give us more reward.
Conclusion
In this project I have simulated a static light field and introduced agents to swarm
around and find the darkest place. We can see how the simple interactions between
agents benefits the whole swarm. I have implemented an algorithm to optimize ğ‘Ÿ
ğ‘œ
, ğ‘Ÿ
ğ‘
using reinforcement learning which was given in the paper. We can see how optimizing
ğ‘Ÿ can help in effectively sensing the gradient.
